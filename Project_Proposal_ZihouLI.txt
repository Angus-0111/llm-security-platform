A Web-based Platform for Simulating and Visualizing Attacks on Educational LLM ToolsZihou LI1656268IntroductionThe rapid integration of Large Language Models (LLMs) such as ChatGPT and similar AI tools into educational platforms has brought significant opportunities for enhancing learning and teaching. However, these advancements also introduce new security risks, including prompt injection, adversarial attacks, and potential data leakage, which can compromise the integrity, privacy, and reliability of educational content and interactions.In this project, “educational scenarios” refer to a range of real-world applications where LLMs are integrated into learning environments. These include, but are not limited to, automated essay grading, intelligent tutoring systems, student-teacher chatbots, content generation for courses, and academic integrity tools. The platform will provide attack simulations and case studies tailored to these representative use cases, ensuring relevance to both K-12 and higher education, as well as online and traditional classroom settings.I chose this topic because it aligns with my academic interests, ongoing coursework, and the increasing demand for secure AI applications in the education sector. This project aims to develop a web-based platform that allows users to simulate and visualize common security attacks on LLMs in educational scenarios. By providing an interactive environment to test, analyse, and understand various attack vectors, the platform will help educators, developers, and students recognize vulnerabilities and explore effective mitigation strategies.To ensure the platform is content-rich and educationally relevant, it will integrate a curated knowledge base of LLM security threats, real-world incidents, and attack examples, with a special focus on cases and data relevant to educational settings. This will address the lack of accessible tools and resources for demonstrating and understanding LLM security risks in practical, education-focused contexts. Ultimately, this project seeks to raise awareness, support research, and contribute to the development of safer AI-powered educational technologies.Objectives/Goals* To design and implement a user-friendly web platform that enables simulation of common security attacks on educational LLM tools, such as prompt injection, adversarial input, and data leakage.* To provide clear and interactive visualizations that illustrate the effects of different attacks on LLM outputs, helping users understand the risks and consequences.* To allow users (educators, developers, students) to experiment with custom inputs and attack scenarios in a safe, controlled environment.* To generate automated risk assessments and summary reports based on simulated attack results.* To offer practical recommendations and resources for mitigating LLM security risks in educational settings.* To integrate a knowledge base of LLM security threats, real-world incidents, and educationally relevant case studies, making the platform a comprehensive resource for learning and research.* To raise awareness and support research on AI security in education by making the platform accessible and extensible for future enhancements.Requirements* Functional Requirementso The platform shall allow users to select and simulate different types of LLM security attacks (e.g., prompt injection, adversarial input, data leakage), with pre-set and customizable scenarios tailored for educational contexts such as essay grading, tutoring chatbots, and content generation.o The platform shall provide an interface for users to input custom prompts and attack parameters.o The platform shall display both the original and attacked LLM outputs for comparison, with differences visually highlighted.o The platform shall visualize the impact of attacks using interactive charts, graphs, dashboards, and other visual aids.o The platform shall generate automated risk assessments and summary reports based on simulation results.o The platform shall offer recommendations for mitigating identified security risks.o The platform shall provide access to a curated knowledge base of LLM security threats, real-world incidents, and educational case studies.o The platform shall support user authentication and role-based access (e.g., educator, student, developer) if implemented.* Non-Functional Requirementso The platform should have an intuitive and user-friendly interface.o The system should ensure data privacy and not store sensitive user inputs or outputs.o The platform should be responsive and accessible on both desktop and mobile devices.o The system should be modular and extensible to allow for future addition of new attack types, features, or data sources.o The platform should provide reliable performance, with reasonable response times for simulations and visualizations.o The system should be secure against common web vulnerabilities (e.g., XSS, CSRF).o User authentication and role-based access should be implemented if required, ensuring secure and appropriate access for different user groups.Functionality* Attack Simulation: Users can select from a range of common LLM security attacks (such as prompt injection, adversarial input, and data leakage) and apply them to educational LLM tools, with pre-set educational scenarios (e.g., essay grading, tutoring chatbots, content generation) and custom input options.* Custom Input: Users can enter their own prompts and configure attack parameters to test specific scenarios.* Output Comparison: The system will display both the original LLM response and the response after the simulated attack side by side, with differences highlighted using colour coding, bold text, or tooltips for easy analysis.* Visualization: The platform will present the impact of attacks through interactive visual elements, including:o Risk Assessment Dashboards: Gauges, progress bars, or radar charts to show the severity of each attack.o Statistical Charts: Bar charts, pie charts, or line graphs to illustrate the frequency, success rate, and impact of different attack types across various educational scenarios.o Sensitive Term Highlighting: Automatic detection and visual marking of sensitive or anomalous outputs.o Attack Path Animation: Flow diagrams or step-by-step animations to demonstrate how an attack manipulates the LLM’s behaviour.* Risk Assessment: Automated risk scores and summary reports will be generated based on the results of each simulation, with downloadable or shareable formats.* Recommendations: The system will provide practical suggestions and resources for mitigating identified security risks, tailored to the specific educational scenario and attack type.* Knowledge Base: The platform will feature a searchable, categorized repository of LLM security threats, real-world incidents, and educational case studies, sourced from public databases and research.* News & Incident Aggregation: The platform will aggregate and display recent LLM security news, incidents, and statistics, with a filter for education-related content.* User Management (optional): The platform may include basic user authentication and role-based access to tailor the experience for different user groups (e.g., educators, students, developers).Tech Stack* Frontend: React.js for building interactive user interfaces.* Backend: Node.js for scalable server-side development.* LLM Integration: APIs such as OpenAI (e.g., ChatGPT), Gemini, Claude, DeepSeek, with the final provider to be determined based on project needs and resource availability.* Database: MySQL or MongoDB, depending on the nature of the data and access patterns.* Authentication: Standard mechanisms such as JWT or OAuth2 for secure user access, if user management is implemented.* Deployment: AWS cloud platform, consistent with practices taught in COMPX527. * Data Integration: Publicly available LLM security incident datasets and repositories, to be selected as appropriate.Timeline/ScheduleWeek 1 (Proposal Week):* Conduct background research on LLM security in education* Finalize project topic and objectives* Write and submit project proposalWeek 2:* Set up project repository and development environment* Finalize detailed requirements and system architecture* Create initial wireframes and UI prototypesWeek 3:* Develop frontend UI prototype* Set up backend project structure* Prepare documentation for initial design decisionsWeek 4:* Integrate basic LLM API functionality (e.g., OpenAI, Gemini, Claude, DeepSeek)* Enable simple prompt input and output displayWeek 5:* Implement prompt injection attack simulation module* Demonstrate basic attack scenario and output comparisonWeek 6:* Develop output comparison feature* Build initial visualization module for attack resultsWeek 7:* Implement adversarial input attack simulation* Refine attack simulation logic and user interfaceWeek 8:* Add risk assessment and automated report generation features* Test and document risk scoring logicWeek 9:* Integrate and display knowledge base and news aggregation modules* Enhance visualization module with interactive charts/graphs* Improve UI/UX based on feedbackWeek 10:* Enable user customization of inputs and attack parameters* Conduct user testing and collect feedback* Integrate all modules, conduct comprehensive testing* Fix bugs, optimize performance, and prepare for final presentationDeployment Plan* The application will be containerized using Docker to ensure consistency across development and production environments.* The backend and frontend components will be deployed on AWS cloud platform.* Environment variables and API keys (e.g., for LLM integration) will be securely managed using AWS secret management tools.* The database (e.g., MongoDB Atlas or AWS RDS) will be hosted in the cloud to ensure scalability and reliability.* After deployment, the platform will be accessible via a public URL, allowing users to interact with the system from any device with internet access.* Regular updates and bug fixes will be managed through the development workflow.* This deployment approach ensures that the platform is accessible, maintainable, and scalable, and allows for easy demonstration and user testing throughout the project.Testing Plan* Unit Testing: Individual components and functions (both frontend and backend) will be tested using appropriate testing frameworks (e.g., Jest for JavaScript) to ensure correctness and reliability.* Integration Testing: The interaction between frontend, backend, and LLM APIs will be tested to verify that data flows and system integration work as intended.* User Acceptance Testing: The platform will be tested by potential users (e.g., classmates, educators) to gather feedback on usability, functionality, and user experience. Issues identified will be addressed through iterative improvements.* Security Testing: Simulated attack scenarios will be used to verify that the platform correctly demonstrates LLM vulnerabilities and that sensitive data is handled securely.* Performance Testing: The system’s response time and stability will be evaluated under typical and peak usage conditions to ensure a smooth user experience.* Bug Tracking and Resolution: All identified bugs and issues will be tracked using a version control system (e.g., GitHub Issues) and resolved promptly.* Content Validation: The accuracy and relevance of the integrated knowledge base and news feeds will be periodically reviewed and updated.* Comprehensive testing throughout the development process will help ensure that the platform is robust, user-friendly, and reliable.ConclusionIn summary, this project aims to develop a web-based platform that enables users to simulate and visualize security attacks on educational LLM tools, enriched with a curated knowledge base and real-world incident data. By providing an interactive environment for testing, analysis, and risk assessment, the platform will help educators, developers, and students better understand the vulnerabilities of LLMs and explore effective mitigation strategies.The platform is designed to be extensible, supporting future enhancements and research in the field of AI security in education. Through this project, I hope to contribute to the advancement of AI security awareness in education, support research in this critical area, and develop practical skills in web development, cybersecurity, and user-centred design. Ultimately, the platform aspires to promote safer and more trustworthy AI-powered educational technologies.